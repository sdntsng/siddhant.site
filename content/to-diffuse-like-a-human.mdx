---
title: "To Diffuse Like a Human"
publishedAt: "2025-12-22"
summary: "Why diffusion models lagged behind LLMs, and why they are catching up."
image: "/blog/diffusion-rise.png"
tags: ["ai", "misc"]
---

There is a distinct duality in the current generative AI landscape. On one side, we have the Large Language Models (LLMs), the articulate, reasoning giants built on the Transformer architecture. On the other, the Diffusion models, the dreamers, generating continuous data like images, audio, and video from noise.

For the past few years, the narrative has been dominated by the former. It is easy to see why. Text is discrete, abundant, and functionally infinite on the internet. The objective function of "predict the next token" is elegantly simple to evaluate, and the scaling laws derived from it provided a predictable roadmap for capital allocation. We built chat interfaces because we had the distribution networks for text already laid out.

But while the world was captivated by the autoregressive turn, diffusion models have been quietly maturing, often playing by a completely different set of rules.

<Image
  src="/blog/diffusion-rise.png"
  alt="Abstract watercolor representing the convergence of structured code and fluid diffusion"
  width={1024}
  height={1024}
/>

## noise is just entropy in reverse

Unlike the rigid, step-by-step assembly of language models, diffusion operates on a principle closer to thermodynamics. It is a process of iterative denoising, learning to reverse entropy. This distinction is not merely architectural; it is philosophical.

While LLMs are often critiqued for "memorization," diffusion models offer a different perspective on learning. As researchers note, memorization and generalization in this high-dimensional space are not necessarily opposites. Diffusion works by repairing "broken" points in the data distribution, pushing them back onto the manifold of reality. It is a more continuous, fluid approach to modeling information.

This difference has historically been a disadvantage. Generating an image via iterative denoising is computationally heavier, often 5â€“10x more than generating a comparable coherent string of text. Furthermore, the quality of a generated image is notoriously subjective. There is no simple "accuracy" metric for art, making progress harder to quantify and "SOTA" harder to claim.

## it's just better engineering

However, the winds are shifting. Purely from a scalability and engineering perspective, diffusion models are beginning to show superior traits for the next phase of AI.

The most compelling argument is one of control. Autoregressive models are notoriously difficult to steer; once the sequence begins, the probability distribution is cast. Diffusion, by contrast, is designed for conditional generation. It allows us to intervene in the noise, to guide the trajectory of the generation at inference time. This is why we are seeing rapid advances in "consistency models", techniques that distill the multi-step diffusion process into fewer, more efficient steps without losing that steerability.

There is also a growing sentiment in the research community that the future of *language* itself might be diffusion-based. Current LLMs predict the next word, but human thought is rarely so linear. We think in holistic structures, in gestalts that we then serialize into speech. A language model based on diffusion principles could theoretically generate entire coherent thought structures at once.

<Tweet id="2003105134285353086" />

## bridging the gap

We are already seeing the bridges built. Models like Sora are not just "video generators"; they are world simulators that understand physics through the lens of diffusion. Tools like Runway and Pika are proving that with enough engineering, the probabilistic nature of diffusion can be tamed into reliable production pipelines.

The lag in diffusion's maturity was never about a lack of potential, it was about the difficulty of the problem. Text was the low-hanging fruit. But as we move toward a multimodal future, where AI needs to understand the continuous signal of the physical world as well as the discrete symbols of language, the "quiet" rise of diffusion looks less like a sub-plot and more like the main event.

We are likely just scratching the surface. When verifyable "truth" is arguably exhausted in text training data, the infinite, continuous variations of the physical world offer a new frontier for scaling. The next great leap in AI might not come from reading more books, but from dreaming better dreams.
